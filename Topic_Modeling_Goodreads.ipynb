{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "import xlrd\n",
    "import string\n",
    "from nltk.corpus import stopwords\n",
    "from random import shuffle\n",
    "import csv\n",
    "from gensim import corpora\n",
    "\n",
    "import numpy as np\n",
    "from scipy.spatial import distance\n",
    "\n",
    "# nltk.download('stopwords')  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from gensim import corpora\n",
    "import pickle\n",
    "import gensim\n",
    "import copy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Read-in list of goodreads data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The dataset contains 1,000 books grabbed from the good reads data, organized into a dictionary. One can apply the same model to thw whole dataset (or any other dataset one wants). Here we grab the tags and book description for the book."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"thousand_dict.pickle\", \"rb\") as f:\n",
    "    Goodreads_dic = pickle.load(f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next we train a topic model for all the books. I also print the log perplexity for model comparison. One should definitely want to do cross-validation over different hyper-parameter combos to select the models. Here I am just use some numbers that seem to work."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def book_topic_gen(book_dic, NUM_TOPICS = 50, NUM_PASSES = 15, seed = 123):\n",
    "    dictionary = corpora.Dictionary(book_dic.values())\n",
    "    corpus = [dictionary.doc2bow(text) for text in book_dic.values()]\n",
    "    ldamodel = gensim.models.ldamodel.LdaModel(corpus, num_topics = NUM_TOPICS, id2word=dictionary, passes = NUM_PASSES, random_state = seed)\n",
    "    with open('GoodReads/' + str(NUM_TOPICS) + '_Topics/GoodReads_model_' + str(NUM_TOPICS) + '.pkl', 'wb') as f:\n",
    "        pickle.dump(ldamodel, f, pickle.HIGHEST_PROTOCOL)\n",
    "    print(ldamodel.log_perplexity(corpus))\n",
    "    return(ldamodel)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Hyper-parameter for training and for display\n",
    "NUM_TOPICS = 50\n",
    "NUM_PASSES = 15\n",
    "NUM_WORDS = 10\n",
    "seed = 123 \n",
    "\n",
    "ldamodel = book_topic_gen(book_dic, NUM_TOPICS, NUM_PASSES, seed)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Checking models and getting embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('GoodReads/' + str(NUM_TOPICS) + '_Topics/GoodReads_model_' + str(NUM_TOPICS) + '.pkl', 'rb') as f:\n",
    "    ldamodel = pickle.load(f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Similar to what we did with twitter data, here we grab and display the topics from books"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_topics(model):\n",
    "    ## Getting the topics with the top words asscociated with this topic\n",
    "    topics = model.print_topics(num_topics=NUM_TOPICS, num_words=NUM_WORDS)\n",
    "        \n",
    "    topics_dic = {}\n",
    "    for i in range(NUM_TOPICS):\n",
    "        topic = topics[i][1].split(\"+\")\n",
    "        topic_dic = {}\n",
    "        for item in topic:\n",
    "            weight = float(item.split(\"*\")[0])\n",
    "            key = item.split(\"*\")[1].split(\"\\\"\")[1]\n",
    "            topic_dic[key] = weight\n",
    "        topics_dic[i] = topic_dic.copy()\n",
    "\n",
    "    return(topics_dic)\n",
    "\n",
    "def print_topics(topic_dic):\n",
    "    for i in topic_dic.keys():\n",
    "        print(\"Topic \" + str(i + 1) + \":\")\n",
    "        for j in topic_dic[i].keys():\n",
    "            print('  ' + j + ' (' + str(topic_dic[i][j]) + ')')  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And generate topic embeddings for each topic."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"../glove_dict.pkl\" , \"rb\") as f:\n",
    "    glove_dic = pickle.load(f)\n",
    "    \n",
    "vocab = glove_dic.keys()\n",
    "\n",
    "def create_embeddings(topic_dic):\n",
    "    vec_dic = {}\n",
    "    for key in topic_dic.keys():\n",
    "        topic = topic_dic[key]\n",
    "        acc = 0\n",
    "        acc_vec = np.zeros(100)\n",
    "        for key_word in topic: \n",
    "            if key_word in vocab:\n",
    "                acc_vec += glove_dic[key_word] * topic[key_word]\n",
    "                acc += topic[key_word]\n",
    "        if acc == 0:\n",
    "            print(key)\n",
    "            print(\"No word found in the vocabulary :(\")  \n",
    "        else:\n",
    "            acc_vec /= acc\n",
    "        vec_dic[key] = acc_vec\n",
    "    return(vec_dic)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "metadata": {},
   "outputs": [],
   "source": [
    "GoodReads_topics_dic = get_topics(ldamodel)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Topic 1:\n",
      "  books (0.081)\n",
      "  children (0.051)\n",
      "  childhood (0.039)\n",
      "  read (0.032)\n",
      "  childrens (0.028)\n",
      "  fiction (0.026)\n",
      "  kids (0.018)\n",
      "  lit (0.017)\n",
      "  favorites (0.017)\n",
      "  kid (0.016)\n",
      "  literature (0.014)\n",
      "  book (0.013)\n",
      "  school (0.013)\n",
      "  library (0.012)\n",
      "  young (0.012)\n",
      "Topic 2:\n",
      "  read (0.044)\n",
      "  manga (0.043)\n",
      "  graphic (0.024)\n",
      "  novels (0.021)\n",
      "  comics (0.018)\n",
      "  books (0.018)\n",
      "  series (0.012)\n",
      "  owned (0.008)\n",
      "  adult (0.007)\n",
      "  mystery (0.007)\n",
      "  comic (0.007)\n",
      "  fiction (0.007)\n",
      "  black (0.007)\n",
      "  favorites (0.006)\n",
      "  stars (0.006)\n",
      "Topic 3:\n",
      "  read (0.0)\n",
      "  books (0.0)\n",
      "  favorites (0.0)\n",
      "  fiction (0.0)\n",
      "  adult (0.0)\n",
      "  memoir (0.0)\n",
      "  biography (0.0)\n",
      "  owned (0.0)\n",
      "  library (0.0)\n",
      "  reading (0.0)\n",
      "  book (0.0)\n",
      "  school (0.0)\n",
      "  audio (0.0)\n",
      "  fantasy (0.0)\n",
      "  favorite (0.0)\n",
      "Topic 4:\n",
      "  food (0.051)\n",
      "  cooking (0.025)\n",
      "  naruto (0.02)\n",
      "  books (0.015)\n",
      "  cookbooks (0.011)\n",
      "  new (0.009)\n",
      "  drink (0.009)\n",
      "  recipes (0.008)\n",
      "  indian (0.007)\n",
      "  booker (0.007)\n",
      "  foodie (0.007)\n",
      "  read (0.006)\n",
      "  chapters (0.006)\n",
      "  joy (0.005)\n",
      "  balram (0.005)\n",
      "Topic 5:\n",
      "  fantasy (0.02)\n",
      "  read (0.017)\n",
      "  books (0.011)\n",
      "  fiction (0.01)\n",
      "  adult (0.007)\n",
      "  stormlight (0.006)\n",
      "  favorite (0.006)\n",
      "  archive (0.005)\n",
      "  sci (0.005)\n",
      "  short (0.005)\n",
      "  walls (0.004)\n",
      "  war (0.004)\n",
      "  story (0.004)\n",
      "  book (0.004)\n",
      "  fi (0.004)\n",
      "Topic 6:\n",
      "  millennium (0.017)\n",
      "  swedish (0.012)\n",
      "  scandinavian (0.01)\n",
      "  salander (0.009)\n",
      "  scandinavia (0.009)\n",
      "  also (0.009)\n",
      "  millenium (0.008)\n",
      "  nordic (0.008)\n",
      "  sweden (0.008)\n",
      "  2011 (0.008)\n",
      "  2010 (0.007)\n",
      "  2012 (0.007)\n",
      "  lisbeth (0.006)\n",
      "  larsson (0.006)\n",
      "  stieg (0.006)\n",
      "Topic 7:\n",
      "  read (0.058)\n",
      "  fantasy (0.051)\n",
      "  books (0.035)\n",
      "  fiction (0.032)\n",
      "  sci (0.021)\n",
      "  fi (0.021)\n",
      "  audio (0.014)\n",
      "  library (0.013)\n",
      "  book (0.013)\n",
      "  adult (0.013)\n",
      "  science (0.012)\n",
      "  scifi (0.011)\n",
      "  owned (0.011)\n",
      "  favorites (0.01)\n",
      "  2015 (0.009)\n",
      "Topic 8:\n",
      "  read (0.0)\n",
      "  books (0.0)\n",
      "  discworld (0.0)\n",
      "  fiction (0.0)\n",
      "  fantasy (0.0)\n",
      "  series (0.0)\n",
      "  book (0.0)\n",
      "  memoir (0.0)\n",
      "  audio (0.0)\n",
      "  adult (0.0)\n",
      "  biography (0.0)\n",
      "  library (0.0)\n",
      "  reading (0.0)\n",
      "  story (0.0)\n",
      "  fi (0.0)\n",
      "Topic 9:\n",
      "  read (0.056)\n",
      "  fiction (0.053)\n",
      "  books (0.046)\n",
      "  book (0.02)\n",
      "  library (0.018)\n",
      "  literature (0.018)\n",
      "  classics (0.016)\n",
      "  1001 (0.015)\n",
      "  favorites (0.014)\n",
      "  american (0.013)\n",
      "  club (0.013)\n",
      "  adult (0.013)\n",
      "  owned (0.012)\n",
      "  reading (0.012)\n",
      "  gilmore (0.012)\n",
      "Topic 10:\n",
      "  mortal (0.042)\n",
      "  instruments (0.041)\n",
      "  read (0.035)\n",
      "  infernal (0.03)\n",
      "  devices (0.029)\n",
      "  city (0.026)\n",
      "  shadowhunters (0.02)\n",
      "  clockwork (0.016)\n",
      "  books (0.016)\n",
      "  angels (0.016)\n",
      "  demons (0.013)\n",
      "  fantasy (0.012)\n",
      "  dark (0.012)\n",
      "  ya (0.012)\n",
      "  artifices (0.012)\n",
      "Topic 11:\n",
      "  adventure (0.026)\n",
      "  fiction (0.018)\n",
      "  books (0.016)\n",
      "  read (0.014)\n",
      "  holmes (0.013)\n",
      "  literature (0.012)\n",
      "  ancient (0.011)\n",
      "  classic (0.01)\n",
      "  classics (0.009)\n",
      "  greek (0.008)\n",
      "  library (0.008)\n",
      "  sherlock (0.007)\n",
      "  watson (0.007)\n",
      "  stories (0.006)\n",
      "  christian (0.006)\n",
      "Topic 12:\n",
      "  history (0.03)\n",
      "  mortmain (0.006)\n",
      "  zinns (0.004)\n",
      "  au (0.004)\n",
      "  institute (0.004)\n",
      "  states (0.004)\n",
      "  united (0.004)\n",
      "  needs (0.004)\n",
      "  politics (0.003)\n",
      "  largely (0.002)\n",
      "  thosewhose (0.002)\n",
      "  sopranos (0.002)\n",
      "  revolutionize (0.002)\n",
      "  activism (0.002)\n",
      "  تاريخ (0.002)\n",
      "Topic 13:\n",
      "  hugo (0.033)\n",
      "  sci (0.024)\n",
      "  fi (0.024)\n",
      "  award (0.023)\n",
      "  winners (0.023)\n",
      "  fiction (0.021)\n",
      "  fantasy (0.018)\n",
      "  read (0.018)\n",
      "  sf (0.017)\n",
      "  books (0.016)\n",
      "  science (0.016)\n",
      "  nebula (0.015)\n",
      "  military (0.012)\n",
      "  pulitzer (0.012)\n",
      "  scifi (0.012)\n",
      "Topic 14:\n",
      "  read (0.026)\n",
      "  books (0.017)\n",
      "  book (0.015)\n",
      "  fiction (0.012)\n",
      "  club (0.01)\n",
      "  finish (0.008)\n",
      "  reads (0.007)\n",
      "  2017 (0.007)\n",
      "  2016 (0.007)\n",
      "  2015 (0.006)\n",
      "  adult (0.006)\n",
      "  favorites (0.006)\n",
      "  world (0.005)\n",
      "  audio (0.004)\n",
      "  story (0.004)\n",
      "Topic 15:\n",
      "  fantasy (0.042)\n",
      "  read (0.032)\n",
      "  books (0.029)\n",
      "  fiction (0.022)\n",
      "  narnia (0.016)\n",
      "  series (0.013)\n",
      "  sci (0.012)\n",
      "  fi (0.012)\n",
      "  adult (0.012)\n",
      "  children (0.011)\n",
      "  de (0.011)\n",
      "  favorites (0.011)\n",
      "  library (0.009)\n",
      "  ya (0.008)\n",
      "  childhood (0.007)\n",
      "Topic 16:\n",
      "  religion (0.016)\n",
      "  non (0.015)\n",
      "  nonfiction (0.013)\n",
      "  blink (0.008)\n",
      "  religious (0.008)\n",
      "  read (0.007)\n",
      "  god (0.007)\n",
      "  faith (0.006)\n",
      "  history (0.006)\n",
      "  decision (0.006)\n",
      "  others (0.005)\n",
      "  book (0.005)\n",
      "  city (0.005)\n",
      "  audio (0.005)\n",
      "  philosophy (0.005)\n",
      "Topic 17:\n",
      "  books (0.027)\n",
      "  fiction (0.025)\n",
      "  read (0.022)\n",
      "  fantasy (0.021)\n",
      "  series (0.011)\n",
      "  library (0.009)\n",
      "  sci (0.008)\n",
      "  fi (0.008)\n",
      "  owned (0.008)\n",
      "  favorites (0.007)\n",
      "  ya (0.007)\n",
      "  adult (0.006)\n",
      "  action (0.006)\n",
      "  adventure (0.005)\n",
      "  maze (0.005)\n",
      "Topic 18:\n",
      "  art (0.104)\n",
      "  books (0.022)\n",
      "  design (0.016)\n",
      "  arts (0.014)\n",
      "  visual (0.014)\n",
      "  read (0.014)\n",
      "  library (0.012)\n",
      "  history (0.011)\n",
      "  creativity (0.009)\n",
      "  architecture (0.009)\n",
      "  artists (0.009)\n",
      "  buy (0.009)\n",
      "  owned (0.008)\n",
      "  photography (0.008)\n",
      "  want (0.008)\n",
      "Topic 19:\n",
      "  read (0.0)\n",
      "  mystery (0.0)\n",
      "  books (0.0)\n",
      "  fiction (0.0)\n",
      "  thriller (0.0)\n",
      "  favorites (0.0)\n",
      "  crime (0.0)\n",
      "  suspense (0.0)\n",
      "  adult (0.0)\n",
      "  series (0.0)\n",
      "  owned (0.0)\n",
      "  book (0.0)\n",
      "  contemporary (0.0)\n",
      "  audio (0.0)\n",
      "  library (0.0)\n",
      "Topic 20:\n",
      "  read (0.068)\n",
      "  school (0.049)\n",
      "  books (0.036)\n",
      "  fiction (0.031)\n",
      "  classics (0.027)\n",
      "  classic (0.023)\n",
      "  literature (0.021)\n",
      "  reading (0.02)\n",
      "  lit (0.018)\n",
      "  library (0.016)\n",
      "  english (0.015)\n",
      "  historical (0.013)\n",
      "  adult (0.012)\n",
      "  owned (0.011)\n",
      "  gilmore (0.011)\n",
      "Topic 21:\n",
      "  fantasy (0.044)\n",
      "  fiction (0.017)\n",
      "  lord (0.015)\n",
      "  tolkien (0.015)\n",
      "  rings (0.014)\n",
      "  books (0.013)\n",
      "  ring (0.013)\n",
      "  middleearth (0.013)\n",
      "  fi (0.012)\n",
      "  sci (0.012)\n",
      "  epic (0.01)\n",
      "  literature (0.009)\n",
      "  favorites (0.009)\n",
      "  jrr (0.008)\n",
      "  r (0.008)\n",
      "Topic 22:\n",
      "  paranormal (0.047)\n",
      "  ya (0.043)\n",
      "  fantasy (0.042)\n",
      "  series (0.041)\n",
      "  read (0.038)\n",
      "  books (0.033)\n",
      "  romance (0.025)\n",
      "  young (0.019)\n",
      "  stars (0.017)\n",
      "  teen (0.014)\n",
      "  vampire (0.013)\n",
      "  love (0.013)\n",
      "  2011 (0.013)\n",
      "  adult (0.012)\n",
      "  2010 (0.011)\n",
      "Topic 23:\n",
      "  shades (0.044)\n",
      "  fifty (0.039)\n",
      "  ana (0.017)\n",
      "  grey (0.017)\n",
      "  trilogy (0.017)\n",
      "  james (0.012)\n",
      "  50 (0.011)\n",
      "  christian (0.01)\n",
      "  dark (0.008)\n",
      "  erotic (0.008)\n",
      "  together (0.006)\n",
      "  romance (0.005)\n",
      "  billionaire (0.005)\n",
      "  christians (0.005)\n",
      "  audiences (0.005)\n",
      "Topic 24:\n",
      "  non (0.044)\n",
      "  self (0.044)\n",
      "  development (0.031)\n",
      "  personal (0.03)\n",
      "  business (0.029)\n",
      "  science (0.026)\n",
      "  read (0.025)\n",
      "  nonfiction (0.023)\n",
      "  philosophy (0.018)\n",
      "  help (0.016)\n",
      "  books (0.016)\n",
      "  improvement (0.015)\n",
      "  psychology (0.015)\n",
      "  growth (0.015)\n",
      "  economics (0.011)\n",
      "Topic 25:\n",
      "  books (0.0)\n",
      "  fantasy (0.0)\n",
      "  fiction (0.0)\n",
      "  series (0.0)\n",
      "  read (0.0)\n",
      "  thriller (0.0)\n",
      "  discworld (0.0)\n",
      "  ya (0.0)\n",
      "  adult (0.0)\n",
      "  young (0.0)\n",
      "  also (0.0)\n",
      "  library (0.0)\n",
      "  owned (0.0)\n",
      "  mystery (0.0)\n",
      "  book (0.0)\n",
      "Topic 26:\n",
      "  gilmore (0.026)\n",
      "  rory (0.013)\n",
      "  challenge (0.009)\n",
      "  girls (0.008)\n",
      "  biography (0.008)\n",
      "  non (0.007)\n",
      "  memoir (0.007)\n",
      "  horse (0.006)\n",
      "  mental (0.006)\n",
      "  reading (0.006)\n",
      "  memoirs (0.006)\n",
      "  seabiscuit (0.005)\n",
      "  racing (0.005)\n",
      "  animal (0.005)\n",
      "  true (0.005)\n",
      "Topic 27:\n",
      "  read (0.026)\n",
      "  fiction (0.02)\n",
      "  series (0.015)\n",
      "  books (0.014)\n",
      "  historical (0.014)\n",
      "  audio (0.008)\n",
      "  book (0.008)\n",
      "  fantasy (0.008)\n",
      "  john (0.008)\n",
      "  outlander (0.008)\n",
      "  time (0.007)\n",
      "  library (0.007)\n",
      "  history (0.007)\n",
      "  favorites (0.007)\n",
      "  adult (0.006)\n",
      "Topic 28:\n",
      "  read (0.059)\n",
      "  books (0.032)\n",
      "  fiction (0.027)\n",
      "  romance (0.024)\n",
      "  book (0.019)\n",
      "  contemporary (0.019)\n",
      "  adult (0.018)\n",
      "  love (0.015)\n",
      "  chick (0.014)\n",
      "  lit (0.013)\n",
      "  2012 (0.013)\n",
      "  2013 (0.012)\n",
      "  library (0.011)\n",
      "  reads (0.011)\n",
      "  favorites (0.01)\n",
      "Topic 29:\n",
      "  read (0.04)\n",
      "  series (0.03)\n",
      "  batman (0.023)\n",
      "  books (0.023)\n",
      "  fantasy (0.017)\n",
      "  comics (0.016)\n",
      "  fiction (0.016)\n",
      "  foundation (0.015)\n",
      "  novels (0.013)\n",
      "  graphic (0.012)\n",
      "  sci (0.012)\n",
      "  fi (0.011)\n",
      "  science (0.011)\n",
      "  asimov (0.009)\n",
      "  order (0.008)\n",
      "Topic 30:\n",
      "  ss (0.018)\n",
      "  cavalier (0.013)\n",
      "  mystery (0.01)\n",
      "  series (0.01)\n",
      "  short (0.009)\n",
      "  kinsey (0.008)\n",
      "  scarpetta (0.007)\n",
      "  detective (0.006)\n",
      "  read (0.006)\n",
      "  thriller (0.006)\n",
      "  mar (0.006)\n",
      "  shift (0.006)\n",
      "  millhone (0.006)\n",
      "  city (0.006)\n",
      "  nv (0.006)\n",
      "Topic 31:\n",
      "  read (0.069)\n",
      "  books (0.058)\n",
      "  ya (0.032)\n",
      "  fiction (0.027)\n",
      "  contemporary (0.016)\n",
      "  favorites (0.015)\n",
      "  adult (0.015)\n",
      "  2014 (0.015)\n",
      "  stars (0.014)\n",
      "  young (0.014)\n",
      "  library (0.014)\n",
      "  reads (0.014)\n",
      "  book (0.013)\n",
      "  favorite (0.013)\n",
      "  2015 (0.013)\n",
      "Topic 32:\n",
      "  german (0.008)\n",
      "  scent (0.005)\n",
      "  capturing (0.002)\n",
      "  oils (0.002)\n",
      "  perfumethe (0.002)\n",
      "  herbs (0.002)\n",
      "  depravity (0.002)\n",
      "  perfumer (0.002)\n",
      "  decipher (0.002)\n",
      "  jeanbaptiste (0.002)\n",
      "  smells (0.002)\n",
      "  mixing (0.002)\n",
      "  giftan (0.002)\n",
      "  odors (0.002)\n",
      "  evermoreterrifying (0.002)\n",
      "Topic 33:\n",
      "  cleopatra (0.012)\n",
      "  ancient (0.012)\n",
      "  would (0.01)\n",
      "  history (0.009)\n",
      "  memoir (0.008)\n",
      "  could (0.007)\n",
      "  rome (0.007)\n",
      "  biography (0.006)\n",
      "  marley (0.006)\n",
      "  world (0.006)\n",
      "  us (0.005)\n",
      "  egypt (0.005)\n",
      "  toph (0.005)\n",
      "  mouth (0.005)\n",
      "  drive (0.004)\n",
      "Topic 34:\n",
      "  moss (0.01)\n",
      "  mccarthy (0.007)\n",
      "  cormac (0.005)\n",
      "  men (0.004)\n",
      "  strips (0.002)\n",
      "  pickup (0.002)\n",
      "  zones (0.002)\n",
      "  containas (0.002)\n",
      "  truck (0.002)\n",
      "  lawin (0.002)\n",
      "  llewellyn (0.002)\n",
      "  concerns (0.002)\n",
      "  rustlers (0.002)\n",
      "  bellcan (0.002)\n",
      "  livesmccarthy (0.002)\n",
      "Topic 35:\n",
      "  thriller (0.083)\n",
      "  mystery (0.075)\n",
      "  suspense (0.046)\n",
      "  crime (0.044)\n",
      "  books (0.04)\n",
      "  fiction (0.039)\n",
      "  read (0.028)\n",
      "  library (0.019)\n",
      "  favorites (0.018)\n",
      "  thrillers (0.016)\n",
      "  contemporary (0.014)\n",
      "  owned (0.014)\n",
      "  audio (0.013)\n",
      "  book (0.013)\n",
      "  adult (0.011)\n",
      "Topic 36:\n",
      "  read (0.036)\n",
      "  fiction (0.028)\n",
      "  book (0.027)\n",
      "  books (0.026)\n",
      "  club (0.015)\n",
      "  audio (0.012)\n",
      "  library (0.012)\n",
      "  favorites (0.01)\n",
      "  american (0.009)\n",
      "  finish (0.009)\n",
      "  adult (0.009)\n",
      "  family (0.008)\n",
      "  non (0.008)\n",
      "  owned (0.008)\n",
      "  history (0.007)\n",
      "Topic 37:\n",
      "  ender (0.03)\n",
      "  enders (0.025)\n",
      "  series (0.021)\n",
      "  short (0.021)\n",
      "  saga (0.02)\n",
      "  fantasy (0.018)\n",
      "  shadow (0.017)\n",
      "  war (0.017)\n",
      "  fiction (0.015)\n",
      "  stories (0.015)\n",
      "  books (0.015)\n",
      "  read (0.013)\n",
      "  order (0.013)\n",
      "  universe (0.013)\n",
      "  first (0.012)\n",
      "Topic 38:\n",
      "  discworld (0.057)\n",
      "  series (0.035)\n",
      "  fantasy (0.027)\n",
      "  read (0.022)\n",
      "  books (0.014)\n",
      "  order (0.013)\n",
      "  collection (0.012)\n",
      "  found (0.012)\n",
      "  story (0.012)\n",
      "  terry (0.012)\n",
      "  also (0.011)\n",
      "  fiction (0.009)\n",
      "  death (0.009)\n",
      "  reading (0.009)\n",
      "  short (0.009)\n",
      "Topic 39:\n",
      "  memoir (0.057)\n",
      "  biography (0.049)\n",
      "  read (0.047)\n",
      "  books (0.035)\n",
      "  autobiography (0.029)\n",
      "  memoirs (0.028)\n",
      "  non (0.026)\n",
      "  biographies (0.026)\n",
      "  book (0.018)\n",
      "  bio (0.018)\n",
      "  nonfiction (0.018)\n",
      "  library (0.015)\n",
      "  audio (0.014)\n",
      "  fiction (0.013)\n",
      "  reading (0.011)\n",
      "Topic 40:\n",
      "  read (0.026)\n",
      "  mercy (0.02)\n",
      "  books (0.019)\n",
      "  fiction (0.015)\n",
      "  falls (0.014)\n",
      "  short (0.006)\n",
      "  school (0.006)\n",
      "  fantasy (0.006)\n",
      "  library (0.006)\n",
      "  audio (0.006)\n",
      "  stories (0.005)\n",
      "  book (0.005)\n",
      "  adult (0.005)\n",
      "  grace (0.005)\n",
      "  classic (0.005)\n",
      "Topic 41:\n",
      "  read (0.053)\n",
      "  graphic (0.043)\n",
      "  comics (0.035)\n",
      "  novels (0.027)\n",
      "  books (0.026)\n",
      "  christian (0.024)\n",
      "  comic (0.016)\n",
      "  fiction (0.014)\n",
      "  library (0.012)\n",
      "  novel (0.012)\n",
      "  manga (0.009)\n",
      "  owned (0.008)\n",
      "  favorites (0.008)\n",
      "  adult (0.007)\n",
      "  book (0.007)\n",
      "Topic 42:\n",
      "  read (0.023)\n",
      "  fiction (0.018)\n",
      "  books (0.018)\n",
      "  club (0.01)\n",
      "  book (0.009)\n",
      "  library (0.007)\n",
      "  novel (0.007)\n",
      "  2013 (0.006)\n",
      "  adult (0.006)\n",
      "  novels (0.006)\n",
      "  favorites (0.006)\n",
      "  historical (0.005)\n",
      "  audio (0.005)\n",
      "  favorite (0.004)\n",
      "  به (0.004)\n",
      "Topic 43:\n",
      "  history (0.029)\n",
      "  lincoln (0.02)\n",
      "  war (0.014)\n",
      "  civil (0.011)\n",
      "  fiction (0.008)\n",
      "  abraham (0.008)\n",
      "  american (0.007)\n",
      "  historical (0.007)\n",
      "  read (0.006)\n",
      "  us (0.006)\n",
      "  presidents (0.006)\n",
      "  goodwin (0.006)\n",
      "  political (0.006)\n",
      "  non (0.005)\n",
      "  soldiers (0.005)\n",
      "Topic 44:\n",
      "  read (0.034)\n",
      "  books (0.034)\n",
      "  fiction (0.022)\n",
      "  library (0.011)\n",
      "  adult (0.01)\n",
      "  fantasy (0.01)\n",
      "  favorites (0.01)\n",
      "  book (0.009)\n",
      "  ya (0.009)\n",
      "  owned (0.008)\n",
      "  series (0.007)\n",
      "  fi (0.006)\n",
      "  sci (0.006)\n",
      "  favorite (0.005)\n",
      "  movie (0.005)\n",
      "Topic 45:\n",
      "  read (0.0)\n",
      "  fiction (0.0)\n",
      "  books (0.0)\n",
      "  adult (0.0)\n",
      "  book (0.0)\n",
      "  library (0.0)\n",
      "  novels (0.0)\n",
      "  owned (0.0)\n",
      "  favorites (0.0)\n",
      "  graphic (0.0)\n",
      "  comics (0.0)\n",
      "  lit (0.0)\n",
      "  novel (0.0)\n",
      "  school (0.0)\n",
      "  contemporary (0.0)\n",
      "Topic 46:\n",
      "  read (0.0)\n",
      "  books (0.0)\n",
      "  series (0.0)\n",
      "  paranormal (0.0)\n",
      "  ya (0.0)\n",
      "  fantasy (0.0)\n",
      "  fiction (0.0)\n",
      "  adult (0.0)\n",
      "  night (0.0)\n",
      "  la (0.0)\n",
      "  young (0.0)\n",
      "  owned (0.0)\n",
      "  romance (0.0)\n",
      "  favorites (0.0)\n",
      "  house (0.0)\n",
      "Topic 47:\n",
      "  mystery (0.028)\n",
      "  read (0.028)\n",
      "  books (0.022)\n",
      "  series (0.021)\n",
      "  thriller (0.019)\n",
      "  fiction (0.016)\n",
      "  suspense (0.014)\n",
      "  crime (0.014)\n",
      "  audio (0.012)\n",
      "  book (0.011)\n",
      "  library (0.009)\n",
      "  favorites (0.007)\n",
      "  novels (0.007)\n",
      "  patterson (0.007)\n",
      "  mysteries (0.006)\n",
      "Topic 48:\n",
      "  books (0.043)\n",
      "  read (0.037)\n",
      "  king (0.029)\n",
      "  fiction (0.024)\n",
      "  horror (0.023)\n",
      "  stephen (0.019)\n",
      "  library (0.015)\n",
      "  owned (0.012)\n",
      "  book (0.012)\n",
      "  adult (0.012)\n",
      "  night (0.008)\n",
      "  audio (0.008)\n",
      "  e (0.008)\n",
      "  house (0.007)\n",
      "  favorites (0.007)\n",
      "Topic 49:\n",
      "  read (0.049)\n",
      "  books (0.03)\n",
      "  series (0.025)\n",
      "  fantasy (0.018)\n",
      "  paranormal (0.018)\n",
      "  ya (0.016)\n",
      "  vampire (0.013)\n",
      "  favorite (0.012)\n",
      "  favorites (0.011)\n",
      "  2014 (0.011)\n",
      "  fiction (0.011)\n",
      "  adult (0.008)\n",
      "  love (0.008)\n",
      "  2013 (0.008)\n",
      "  young (0.008)\n",
      "Topic 50:\n",
      "  books (0.032)\n",
      "  read (0.03)\n",
      "  series (0.021)\n",
      "  percy (0.019)\n",
      "  jackson (0.014)\n",
      "  fantasy (0.013)\n",
      "  heroes (0.01)\n",
      "  olympus (0.01)\n",
      "  children (0.009)\n",
      "  ya (0.009)\n",
      "  favorites (0.008)\n",
      "  favorite (0.008)\n",
      "  gods (0.008)\n",
      "  fiction (0.008)\n",
      "  mythology (0.008)\n"
     ]
    }
   ],
   "source": [
    "print_topics(GoodReads_topics_dic)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "GoodReads_topics_vec = create_embeddings(GoodReads_topics_dic)\n",
    "with open('GoodReads/' + str(NUM_TOPICS) + '_Topics/GoodReads_topics_' + str(NUM_TOPICS) + '.pkl', 'wb') as f:\n",
    "    pickle.dump(GoodReads_topics_vec, f, pickle.HIGHEST_PROTOCOL)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Save the weighting on each topics for each book"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This creates a dictionary that marks what is each book's weighting on the topics. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ids = list(Goodreads_dic.keys())\n",
    "cnt = 0\n",
    "weight_dic = {}\n",
    "for l in ldamodel[corpus]:\n",
    "    weight_dic[ids[cnt]] = l\n",
    "    cnt += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 145,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('book_weights_sample.pkl', 'wb') as f:\n",
    "    pickle.dump(weight_dic, f, pickle.HIGHEST_PROTOCOL)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 148,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(14, 0.63524973), (16, 0.042723272), (30, 0.3104762)]"
      ]
     },
     "execution_count": 148,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "weight_dic[3]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Calculate the distance between tweets and book topics"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Store a dictionary that contains the distance between the twitter topics to each of the book topic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 149,
   "metadata": {},
   "outputs": [],
   "source": [
    "def topic_tweet_distance(tweet_dic):\n",
    "    dist_dic = {}\n",
    "    for key in tweet_dic.keys():\n",
    "        topic_distance = np.zeros(NUM_TOPICS)\n",
    "        vec = tweet_dic[key]\n",
    "        for i in range(NUM_TOPICS):\n",
    "            goodreads_vec = GoodReads_topics_vec[i]\n",
    "            if goodreads_vec.any(): \n",
    "                topic_distance[i] = 1 - distance.cosine(vec, goodreads_vec)\n",
    "        dist_dic[key] = topic_distance\n",
    "    return(dist_dic)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 150,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "## Load tweet topics\n",
    "handle = \"twitter_handle1\"\n",
    "with open(handle + '_vec_' + str(5) + '.pkl', 'rb') as f:\n",
    "    H1_tweet_dic = pickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 151,
   "metadata": {},
   "outputs": [],
   "source": [
    "H1_tweet_dist = topic_tweet_distance(H1_tweet_dic)\n",
    "with open('GoodReads/' + str(NUM_TOPICS) + '_Topics/' + handle + '_topic_dist_' + str(NUM_TOPICS) + '.pkl', 'wb') as f:\n",
    "    pickle.dump(H1_tweet_dist, f, pickle.HIGHEST_PROTOCOL)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
